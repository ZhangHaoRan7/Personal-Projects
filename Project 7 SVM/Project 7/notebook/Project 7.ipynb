{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi-class and Multi-Label Classification Using Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import sklearn.metrics\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import itertools\n",
    "import copy\n",
    "import os\n",
    "import shutil\n",
    "import urllib.request\n",
    "import csv\n",
    "import pandas as pd\n",
    "from scipy.stats import bootstrap\n",
    "import statistics\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, hamming_loss,silhouette_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "import os\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from datetime import datetime\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import hamming_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Download the Anuran Calls (MFCCs) Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MFCCs_ 1</th>\n",
       "      <th>MFCCs_ 2</th>\n",
       "      <th>MFCCs_ 3</th>\n",
       "      <th>MFCCs_ 4</th>\n",
       "      <th>MFCCs_ 5</th>\n",
       "      <th>MFCCs_ 6</th>\n",
       "      <th>MFCCs_ 7</th>\n",
       "      <th>MFCCs_ 8</th>\n",
       "      <th>MFCCs_ 9</th>\n",
       "      <th>MFCCs_10</th>\n",
       "      <th>...</th>\n",
       "      <th>MFCCs_17</th>\n",
       "      <th>MFCCs_18</th>\n",
       "      <th>MFCCs_19</th>\n",
       "      <th>MFCCs_20</th>\n",
       "      <th>MFCCs_21</th>\n",
       "      <th>MFCCs_22</th>\n",
       "      <th>Family</th>\n",
       "      <th>Genus</th>\n",
       "      <th>Species</th>\n",
       "      <th>RecordID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.152936</td>\n",
       "      <td>-0.105586</td>\n",
       "      <td>0.200722</td>\n",
       "      <td>0.317201</td>\n",
       "      <td>0.260764</td>\n",
       "      <td>0.100945</td>\n",
       "      <td>-0.150063</td>\n",
       "      <td>-0.171128</td>\n",
       "      <td>0.124676</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108351</td>\n",
       "      <td>-0.077623</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>0.057684</td>\n",
       "      <td>0.118680</td>\n",
       "      <td>0.014038</td>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraAndre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.171534</td>\n",
       "      <td>-0.098975</td>\n",
       "      <td>0.268425</td>\n",
       "      <td>0.338672</td>\n",
       "      <td>0.268353</td>\n",
       "      <td>0.060835</td>\n",
       "      <td>-0.222475</td>\n",
       "      <td>-0.207693</td>\n",
       "      <td>0.170883</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090974</td>\n",
       "      <td>-0.056510</td>\n",
       "      <td>-0.035303</td>\n",
       "      <td>0.020140</td>\n",
       "      <td>0.082263</td>\n",
       "      <td>0.029056</td>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraAndre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.152317</td>\n",
       "      <td>-0.082973</td>\n",
       "      <td>0.287128</td>\n",
       "      <td>0.276014</td>\n",
       "      <td>0.189867</td>\n",
       "      <td>0.008714</td>\n",
       "      <td>-0.242234</td>\n",
       "      <td>-0.219153</td>\n",
       "      <td>0.232538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050691</td>\n",
       "      <td>-0.023590</td>\n",
       "      <td>-0.066722</td>\n",
       "      <td>-0.025083</td>\n",
       "      <td>0.099108</td>\n",
       "      <td>0.077162</td>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraAndre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.224392</td>\n",
       "      <td>0.118985</td>\n",
       "      <td>0.329432</td>\n",
       "      <td>0.372088</td>\n",
       "      <td>0.361005</td>\n",
       "      <td>0.015501</td>\n",
       "      <td>-0.194347</td>\n",
       "      <td>-0.098181</td>\n",
       "      <td>0.270375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136009</td>\n",
       "      <td>-0.177037</td>\n",
       "      <td>-0.130498</td>\n",
       "      <td>-0.054766</td>\n",
       "      <td>-0.018691</td>\n",
       "      <td>0.023954</td>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraAndre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.087817</td>\n",
       "      <td>-0.068345</td>\n",
       "      <td>0.306967</td>\n",
       "      <td>0.330923</td>\n",
       "      <td>0.249144</td>\n",
       "      <td>0.006884</td>\n",
       "      <td>-0.265423</td>\n",
       "      <td>-0.172700</td>\n",
       "      <td>0.266434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048885</td>\n",
       "      <td>-0.053074</td>\n",
       "      <td>-0.088550</td>\n",
       "      <td>-0.031346</td>\n",
       "      <td>0.108610</td>\n",
       "      <td>0.079244</td>\n",
       "      <td>Leptodactylidae</td>\n",
       "      <td>Adenomera</td>\n",
       "      <td>AdenomeraAndre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7190</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.554504</td>\n",
       "      <td>-0.337717</td>\n",
       "      <td>0.035533</td>\n",
       "      <td>0.034511</td>\n",
       "      <td>0.443451</td>\n",
       "      <td>0.093889</td>\n",
       "      <td>-0.100753</td>\n",
       "      <td>0.037087</td>\n",
       "      <td>0.081075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069430</td>\n",
       "      <td>0.071001</td>\n",
       "      <td>0.021591</td>\n",
       "      <td>0.052449</td>\n",
       "      <td>-0.021860</td>\n",
       "      <td>-0.079860</td>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Scinax</td>\n",
       "      <td>ScinaxRuber</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7191</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.517273</td>\n",
       "      <td>-0.370574</td>\n",
       "      <td>0.030673</td>\n",
       "      <td>0.068097</td>\n",
       "      <td>0.402890</td>\n",
       "      <td>0.096628</td>\n",
       "      <td>-0.116460</td>\n",
       "      <td>0.063727</td>\n",
       "      <td>0.089034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061127</td>\n",
       "      <td>0.068978</td>\n",
       "      <td>0.017745</td>\n",
       "      <td>0.046461</td>\n",
       "      <td>-0.015418</td>\n",
       "      <td>-0.101892</td>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Scinax</td>\n",
       "      <td>ScinaxRuber</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7192</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.582557</td>\n",
       "      <td>-0.343237</td>\n",
       "      <td>0.029468</td>\n",
       "      <td>0.064179</td>\n",
       "      <td>0.385596</td>\n",
       "      <td>0.114905</td>\n",
       "      <td>-0.103317</td>\n",
       "      <td>0.070370</td>\n",
       "      <td>0.081317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082474</td>\n",
       "      <td>0.077771</td>\n",
       "      <td>-0.009688</td>\n",
       "      <td>0.027834</td>\n",
       "      <td>-0.000531</td>\n",
       "      <td>-0.080425</td>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Scinax</td>\n",
       "      <td>ScinaxRuber</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7193</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.519497</td>\n",
       "      <td>-0.307553</td>\n",
       "      <td>-0.004922</td>\n",
       "      <td>0.072865</td>\n",
       "      <td>0.377131</td>\n",
       "      <td>0.086866</td>\n",
       "      <td>-0.115799</td>\n",
       "      <td>0.056979</td>\n",
       "      <td>0.089316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051796</td>\n",
       "      <td>0.069073</td>\n",
       "      <td>0.017963</td>\n",
       "      <td>0.041803</td>\n",
       "      <td>-0.027911</td>\n",
       "      <td>-0.096895</td>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Scinax</td>\n",
       "      <td>ScinaxRuber</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7194</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.508833</td>\n",
       "      <td>-0.324106</td>\n",
       "      <td>0.062068</td>\n",
       "      <td>0.078211</td>\n",
       "      <td>0.397188</td>\n",
       "      <td>0.094596</td>\n",
       "      <td>-0.117672</td>\n",
       "      <td>0.058874</td>\n",
       "      <td>0.076180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061455</td>\n",
       "      <td>0.072983</td>\n",
       "      <td>-0.003980</td>\n",
       "      <td>0.031560</td>\n",
       "      <td>-0.029355</td>\n",
       "      <td>-0.087910</td>\n",
       "      <td>Hylidae</td>\n",
       "      <td>Scinax</td>\n",
       "      <td>ScinaxRuber</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7195 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MFCCs_ 1  MFCCs_ 2  MFCCs_ 3  MFCCs_ 4  MFCCs_ 5  MFCCs_ 6  MFCCs_ 7  \\\n",
       "0          1.0  0.152936 -0.105586  0.200722  0.317201  0.260764  0.100945   \n",
       "1          1.0  0.171534 -0.098975  0.268425  0.338672  0.268353  0.060835   \n",
       "2          1.0  0.152317 -0.082973  0.287128  0.276014  0.189867  0.008714   \n",
       "3          1.0  0.224392  0.118985  0.329432  0.372088  0.361005  0.015501   \n",
       "4          1.0  0.087817 -0.068345  0.306967  0.330923  0.249144  0.006884   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7190       1.0 -0.554504 -0.337717  0.035533  0.034511  0.443451  0.093889   \n",
       "7191       1.0 -0.517273 -0.370574  0.030673  0.068097  0.402890  0.096628   \n",
       "7192       1.0 -0.582557 -0.343237  0.029468  0.064179  0.385596  0.114905   \n",
       "7193       1.0 -0.519497 -0.307553 -0.004922  0.072865  0.377131  0.086866   \n",
       "7194       1.0 -0.508833 -0.324106  0.062068  0.078211  0.397188  0.094596   \n",
       "\n",
       "      MFCCs_ 8  MFCCs_ 9  MFCCs_10  ...  MFCCs_17  MFCCs_18  MFCCs_19  \\\n",
       "0    -0.150063 -0.171128  0.124676  ... -0.108351 -0.077623 -0.009568   \n",
       "1    -0.222475 -0.207693  0.170883  ... -0.090974 -0.056510 -0.035303   \n",
       "2    -0.242234 -0.219153  0.232538  ... -0.050691 -0.023590 -0.066722   \n",
       "3    -0.194347 -0.098181  0.270375  ... -0.136009 -0.177037 -0.130498   \n",
       "4    -0.265423 -0.172700  0.266434  ... -0.048885 -0.053074 -0.088550   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7190 -0.100753  0.037087  0.081075  ...  0.069430  0.071001  0.021591   \n",
       "7191 -0.116460  0.063727  0.089034  ...  0.061127  0.068978  0.017745   \n",
       "7192 -0.103317  0.070370  0.081317  ...  0.082474  0.077771 -0.009688   \n",
       "7193 -0.115799  0.056979  0.089316  ...  0.051796  0.069073  0.017963   \n",
       "7194 -0.117672  0.058874  0.076180  ...  0.061455  0.072983 -0.003980   \n",
       "\n",
       "      MFCCs_20  MFCCs_21  MFCCs_22           Family      Genus  \\\n",
       "0     0.057684  0.118680  0.014038  Leptodactylidae  Adenomera   \n",
       "1     0.020140  0.082263  0.029056  Leptodactylidae  Adenomera   \n",
       "2    -0.025083  0.099108  0.077162  Leptodactylidae  Adenomera   \n",
       "3    -0.054766 -0.018691  0.023954  Leptodactylidae  Adenomera   \n",
       "4    -0.031346  0.108610  0.079244  Leptodactylidae  Adenomera   \n",
       "...        ...       ...       ...              ...        ...   \n",
       "7190  0.052449 -0.021860 -0.079860          Hylidae     Scinax   \n",
       "7191  0.046461 -0.015418 -0.101892          Hylidae     Scinax   \n",
       "7192  0.027834 -0.000531 -0.080425          Hylidae     Scinax   \n",
       "7193  0.041803 -0.027911 -0.096895          Hylidae     Scinax   \n",
       "7194  0.031560 -0.029355 -0.087910          Hylidae     Scinax   \n",
       "\n",
       "             Species  RecordID  \n",
       "0     AdenomeraAndre         1  \n",
       "1     AdenomeraAndre         1  \n",
       "2     AdenomeraAndre         1  \n",
       "3     AdenomeraAndre         1  \n",
       "4     AdenomeraAndre         1  \n",
       "...              ...       ...  \n",
       "7190     ScinaxRuber        60  \n",
       "7191     ScinaxRuber        60  \n",
       "7192     ScinaxRuber        60  \n",
       "7193     ScinaxRuber        60  \n",
       "7194     ScinaxRuber        60  \n",
       "\n",
       "[7195 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frogs = pd.read_csv('../Data/Anuran Calls (MFCCs)/Frogs.csv')\n",
    "frogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types = []\n",
    "for i in frogs.columns:\n",
    "    types.append(type(frogs[i][2]))\n",
    "pd.unique(types)\n",
    "type(frogs['RecordID'][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Family', 'Genus', 'Species'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate the features (X) and labels (y)\n",
    "X = frogs.iloc[:, :-4]\n",
    "y = frogs[['Family', 'Genus', 'Species']]\n",
    "y_family = frogs['Family']\n",
    "y_genus = frogs['Genus']\n",
    "y_species = frogs['Species']\n",
    "\n",
    "# Split the data into training and testing sets, with 70% for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "y.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Train a classifier for each label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Ratio: 0.9810097267253358\n",
      "Hamming Loss: 0.004000168428144343\n"
     ]
    }
   ],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train one SVM classifier for each label\n",
    "classifiers = {}\n",
    "predictions = pd.DataFrame(index=X_test.index, columns=y.columns)\n",
    "\n",
    "for label in y.columns:\n",
    "    classifier = OneVsRestClassifier(SVC(kernel='rbf', probability=True))\n",
    "    classifier.fit(X_train_scaled, y_train[label])\n",
    "    classifiers[label] = classifier\n",
    "    predictions[label] = classifier.predict(X_test_scaled)\n",
    "\n",
    "# Convert predictions to binary indicator format\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_test_binarized = mlb.fit_transform(y_test.values)\n",
    "predictions_binarized = mlb.transform(predictions.values)\n",
    "\n",
    "# Calculate Exact Match Ratio\n",
    "exact_match_ratio = np.all(y_test_binarized == predictions_binarized, axis=1).mean()\n",
    "print(f\"Exact Match Ratio: {exact_match_ratio}\")\n",
    "\n",
    "# Calculate Hamming Loss\n",
    "hamming_loss_score = hamming_loss(y_test_binarized, predictions_binarized)\n",
    "print(f\"Hamming Loss: {hamming_loss_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Train a SVM for each of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM for label: Family\n",
      "Best parameters: {'C': 10, 'gamma': 0.1}\n",
      "Hamming Loss: 0.007410838351088467\n",
      "Hamming score: 0.9925891616489115\n",
      "EMR: 0.9925891616489115\n",
      "Time taken: 0:03:34.403849\n",
      "Training SVM for label: Genus\n",
      "Best parameters: {'C': 10, 'gamma': 0.1}\n",
      "Hamming Loss: 0.012042612320518759\n",
      "Hamming score: 0.9879573876794813\n",
      "EMR: 0.9879573876794813\n",
      "Time taken: 0:05:24.481931\n",
      "Training SVM for label: Species\n",
      "Best parameters: {'C': 1000, 'gamma': 0.01}\n",
      "Hamming Loss: 0.010653080129689671\n",
      "Hamming score: 0.9893469198703103\n",
      "EMR: 0.9893469198703103\n",
      "Time taken: 0:04:58.755625\n"
     ]
    }
   ],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Train an SVM for each label\n",
    "for label in y.columns:\n",
    "    t1 = datetime.now()  # start timer\n",
    "\n",
    "    print(\"Training SVM for label:\", label)\n",
    "\n",
    "    # Define the parameter grid to search over\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10, 100,1000],\n",
    "        'gamma': [0.01, 0.1, 1, 10]\n",
    "    }\n",
    "\n",
    "    # Perform 10-fold cross validation to find the best parameters\n",
    "    grid_search = GridSearchCV(SVC(kernel='rbf', decision_function_shape='ovr'), param_grid, cv=10)\n",
    "    grid_search.fit(X_train_std, y_train[label])\n",
    "\n",
    "    # Print the best parameters and time taken\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "    # Train the SVM with the best parameters on the full training set\n",
    "    best_svc = grid_search.best_estimator_\n",
    "    best_svc.fit(X_train_std, y_train[label])\n",
    "\n",
    "    # Evaluate the SVM on the test set\n",
    "    y_pred = best_svc.predict(X_test_std)\n",
    "    #print(y_pred)\n",
    "    #print(y_test[label])\n",
    "    # Calculate the Hamming Loss\n",
    "    hl = hamming_loss(y_test[label], y_pred)\n",
    "    print(\"Hamming Loss:\", hl)\n",
    "\n",
    "    # Calculate the Hamming Score\n",
    "    print(\"Hamming score:\", 1-hl)\n",
    "\n",
    "    # Calculate the Exact Match Ratio (EMR)\n",
    "    emr = accuracy_score(y_test[label], y_pred)\n",
    "    print(\"EMR:\", emr)\n",
    "\n",
    "    t2 = datetime.now()  # stop timer\n",
    "    print(\"Time taken:\", t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Repeat 1(b)ii with L1-penalized SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training L1-penalized SVM for label: Family\n",
      "Best penalty weight: {'C': 1}\n",
      "Hamming Loss: 0.07132931912922649\n",
      "Hamming score: 0.9286706808707735\n",
      "EMR: 0.9286706808707735\n",
      "Time taken: 0:00:25.496829\n",
      "Training L1-penalized SVM for label: Genus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haoran Zhang\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Haoran Zhang\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best penalty weight: {'C': 10}\n",
      "Hamming Loss: 0.058360352014821676\n",
      "Hamming score: 0.9416396479851783\n",
      "EMR: 0.9416396479851783\n",
      "Time taken: 0:00:55.197059\n",
      "Training L1-penalized SVM for label: Species\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haoran Zhang\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Haoran Zhang\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best penalty weight: {'C': 10}\n",
      "Hamming Loss: 0.04075961093098657\n",
      "Hamming score: 0.9592403890690134\n",
      "EMR: 0.9592403890690134\n",
      "Time taken: 0:00:59.818733\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Train an L1-penalized SVM for each label\n",
    "for label in y.columns:\n",
    "    t1 = datetime.now()  # start timer\n",
    "\n",
    "    print(\"Training L1-penalized SVM for label:\", label)\n",
    "    \n",
    "    # Define the parameter grid to search over\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10, 100]\n",
    "    }\n",
    "    \n",
    "    # Perform 10-fold cross validation to find the best penalty weight\n",
    "    grid_search = GridSearchCV(LinearSVC(penalty='l1', dual=False, max_iter=10000), param_grid, cv=10)\n",
    "    grid_search.fit(X_train_std, y_train[label])\n",
    "    \n",
    "    # Print the best penalty weight\n",
    "    print(\"Best penalty weight:\", grid_search.best_params_)\n",
    "    \n",
    "    # Train the L1-penalized SVM with the best penalty weight on the full training set\n",
    "    best_svc = grid_search.best_estimator_\n",
    "    best_svc.fit(X_train_std, y_train[label])\n",
    "    \n",
    "    # Evaluate the L1-penalized SVM on the test set\n",
    "    y_pred = best_svc.predict(X_test_std)\n",
    "    # Calculate the Hamming Loss\n",
    "    hl = hamming_loss(y_test[label], y_pred)\n",
    "    print(\"Hamming Loss:\", hl)\n",
    "\n",
    "    # Calculate the Hamming Score\n",
    "    print(\"Hamming score:\", 1-hl)\n",
    "\n",
    "    # Calculate the Exact Match Ratio (EMR)\n",
    "    emr = accuracy_score(y_test[label], y_pred)\n",
    "    print(\"EMR:\", emr)\n",
    "\n",
    "    t2 = datetime.now()  # stop timer\n",
    "    print(\"Time taken:\", t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iv) Repeat 1(b)iii by using SMOTE or any other method for imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM for label: Family\n",
      "Best parameters: {'svm__C': 100, 'svm__gamma': 0.1}\n",
      "Hamming Loss: 0.006947660954145438\n",
      "Hamming score: 0.9930523390458545\n",
      "EMR: 0.9930523390458545\n",
      "Time taken: 0:07:17.491301\n",
      "Training SVM for label: Genus\n",
      "Best parameters: {'svm__C': 10, 'svm__gamma': 0.1}\n",
      "Hamming Loss: 0.010653080129689671\n",
      "Hamming score: 0.9893469198703103\n",
      "EMR: 0.9893469198703103\n",
      "Time taken: 0:25:33.368328\n",
      "Training SVM for label: Species\n",
      "Best parameters: {'svm__C': 10, 'svm__gamma': 0.01}\n",
      "Hamming Loss: 0.012042612320518759\n",
      "Hamming score: 0.9879573876794813\n",
      "EMR: 0.9879573876794813\n",
      "Time taken: 0:30:39.405453\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10, 100],\n",
    "    'svm__gamma': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "\n",
    "# Train an SVM for each label using SMOTE\n",
    "for label in y.columns:\n",
    "    t1 = datetime.now()  # start timer\n",
    "\n",
    "    print(\"Training SVM for label:\", label)\n",
    "    \n",
    "    # Define the pipeline to include SMOTE and SVM\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', SMOTE(sampling_strategy='auto', random_state=42)),\n",
    "        ('svm', SVC(kernel='rbf', decision_function_shape='ovr'))\n",
    "    ])\n",
    "    \n",
    "    # Perform 10-fold cross validation to find the best parameters\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=10)\n",
    "    grid_search.fit(X_train_std, y_train[label])\n",
    "    \n",
    "    # Print the best parameters\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    \n",
    "    # Train the SVM with the best parameters on the full training set\n",
    "    best_svc = grid_search.best_estimator_\n",
    "    best_svc.fit(X_train_std, y_train[label])\n",
    "    \n",
    "    # Evaluate the SVM on the test set\n",
    "    y_pred = best_svc.predict(X_test_std)\n",
    "    # Calculate the Hamming Loss\n",
    "    hl = hamming_loss(y_test[label], y_pred)\n",
    "    print(\"Hamming Loss:\", hl)\n",
    "\n",
    "    # Calculate the Hamming Score\n",
    "    print(\"Hamming score:\", 1-hl)\n",
    "\n",
    "    # Calculate the Exact Match Ratio (EMR)\n",
    "    emr = accuracy_score(y_test[label], y_pred)\n",
    "    print(\"EMR:\", emr)\n",
    "\n",
    "    t2 = datetime.now()  # stop timer\n",
    "    print(\"Time taken:\", t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-Means Clustering on a Multi-Class and Multi-Label Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Use k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration :  0\n",
      "optimal_k is: 4\n",
      "0:00:15.924505\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.634303\n",
      "Iteration :  1\n",
      "optimal_k is: 4\n",
      "0:00:15.375129\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.639665\n",
      "Iteration :  2\n",
      "optimal_k is: 4\n",
      "0:00:15.455514\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.623889\n",
      "Iteration :  3\n",
      "optimal_k is: 4\n",
      "0:00:15.295565\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.619968\n",
      "Iteration :  4\n",
      "optimal_k is: 4\n",
      "0:00:15.217097\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.639246\n",
      "Iteration :  5\n",
      "optimal_k is: 4\n",
      "0:00:15.282032\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.647090\n",
      "Iteration :  6\n",
      "optimal_k is: 4\n",
      "0:00:15.161190\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.602807\n",
      "Iteration :  7\n",
      "optimal_k is: 4\n",
      "0:00:15.221617\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.617712\n",
      "Iteration :  8\n",
      "optimal_k is: 4\n",
      "0:00:15.269297\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.623680\n",
      "Iteration :  9\n",
      "optimal_k is: 4\n",
      "0:00:15.347713\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.616235\n",
      "Iteration :  10\n",
      "optimal_k is: 4\n",
      "0:00:15.312629\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.622965\n",
      "Iteration :  11\n",
      "optimal_k is: 4\n",
      "0:00:15.236931\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.638010\n",
      "Iteration :  12\n",
      "optimal_k is: 4\n",
      "0:00:15.359839\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.631616\n",
      "Iteration :  13\n",
      "optimal_k is: 4\n",
      "0:00:15.985990\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.666868\n",
      "Iteration :  14\n",
      "optimal_k is: 4\n",
      "0:00:15.887066\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.638995\n",
      "Iteration :  15\n",
      "optimal_k is: 4\n",
      "0:00:15.490414\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.641786\n",
      "Iteration :  16\n",
      "optimal_k is: 4\n",
      "0:00:15.277511\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.648381\n",
      "Iteration :  17\n",
      "optimal_k is: 4\n",
      "0:00:15.836386\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.675547\n",
      "Iteration :  18\n",
      "optimal_k is: 4\n",
      "0:00:15.742968\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.651789\n",
      "Iteration :  19\n",
      "optimal_k is: 4\n",
      "0:00:15.923468\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.687927\n",
      "Iteration :  20\n",
      "optimal_k is: 4\n",
      "0:02:09.989459\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.655576\n",
      "Iteration :  21\n",
      "optimal_k is: 4\n",
      "0:00:15.138891\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.638165\n",
      "Iteration :  22\n",
      "optimal_k is: 4\n",
      "0:00:15.663342\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.655892\n",
      "Iteration :  23\n",
      "optimal_k is: 4\n",
      "0:00:15.296154\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.619770\n",
      "Iteration :  24\n",
      "optimal_k is: 4\n",
      "0:00:15.374188\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.628587\n",
      "Iteration :  25\n",
      "optimal_k is: 4\n",
      "0:00:15.165081\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.609440\n",
      "Iteration :  26\n",
      "optimal_k is: 4\n",
      "0:00:15.282851\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.605373\n",
      "Iteration :  27\n",
      "optimal_k is: 4\n",
      "0:00:15.571823\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.618898\n",
      "Iteration :  28\n",
      "optimal_k is: 4\n",
      "0:00:15.231926\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.624175\n",
      "Iteration :  29\n",
      "optimal_k is: 4\n",
      "0:00:14.991718\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.616732\n",
      "Iteration :  30\n",
      "optimal_k is: 4\n",
      "0:00:15.146168\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.644902\n",
      "Iteration :  31\n",
      "optimal_k is: 4\n",
      "0:00:15.080913\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.646930\n",
      "Iteration :  32\n",
      "optimal_k is: 4\n",
      "0:00:15.192102\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.615491\n",
      "Iteration :  33\n",
      "optimal_k is: 4\n",
      "0:00:15.208903\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.631168\n",
      "Iteration :  34\n",
      "optimal_k is: 4\n",
      "0:00:15.366899\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.621081\n",
      "Iteration :  35\n",
      "optimal_k is: 4\n",
      "0:00:15.166511\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.606864\n",
      "Iteration :  36\n",
      "optimal_k is: 4\n",
      "0:00:15.130170\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.643932\n",
      "Iteration :  37\n",
      "optimal_k is: 4\n",
      "0:00:15.118307\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.631064\n",
      "Iteration :  38\n",
      "optimal_k is: 4\n",
      "0:00:15.181273\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.632341\n",
      "Iteration :  39\n",
      "optimal_k is: 4\n",
      "0:00:15.245879\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.633573\n",
      "Iteration :  40\n",
      "optimal_k is: 4\n",
      "0:00:15.217477\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.625197\n",
      "Iteration :  41\n",
      "optimal_k is: 4\n",
      "0:00:15.298929\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.635732\n",
      "Iteration :  42\n",
      "optimal_k is: 4\n",
      "0:00:15.193637\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.626914\n",
      "Iteration :  43\n",
      "optimal_k is: 4\n",
      "0:00:15.157427\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.607570\n",
      "Iteration :  44\n",
      "optimal_k is: 4\n",
      "0:00:15.188420\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.633524\n",
      "Iteration :  45\n",
      "optimal_k is: 4\n",
      "0:00:15.134926\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.628453\n",
      "Iteration :  46\n",
      "optimal_k is: 4\n",
      "0:00:15.071715\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.628517\n",
      "Iteration :  47\n",
      "optimal_k is: 4\n",
      "0:00:15.006897\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.616581\n",
      "Iteration :  48\n",
      "optimal_k is: 4\n",
      "0:00:15.231593\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.631137\n",
      "Iteration :  49\n",
      "optimal_k is: 4\n",
      "0:00:15.179783\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0:00:00.614770\n",
      "Label: Family\n",
      "Mean Hamming distance: 1.0\n",
      "Standard deviation of Hamming distance: 0.0\n",
      "Label: Genus\n",
      "Mean Hamming distance: 1.0\n",
      "Standard deviation of Hamming distance: 0.0\n",
      "Label: Species\n",
      "Mean Hamming distance: 1.0\n",
      "Standard deviation of Hamming distance: 0.0\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Perform the procedure 50 times\n",
    "n_iter = 50\n",
    "hamming_distances = {label: [] for label in y.columns}\n",
    "for i in range(n_iter):\n",
    "    # Find optimal number of clusters using Silhouette score\n",
    "    silh_avg = {}\n",
    "    t1 = datetime.now()  # start timer\n",
    "\n",
    "    for k in range(2,21):\n",
    "        rand_value=random.randint(0, 100)\n",
    "        k_means = KMeans(n_clusters=k,init='k-means++',random_state=rand_value).fit(X)\n",
    "        labels = k_means.labels_\n",
    "        silh_avg.update({k:(metrics.silhouette_score(X, labels))})\n",
    "   \n",
    "    print(\"Iteration : \",i)\n",
    "\n",
    "    # Get optimal k and build K-means clustering with it\n",
    "    optimal_k = max(silh_avg,key=silh_avg.get)\n",
    "    rand_value=random.randint(0, 900)\n",
    "    print('optimal_k is:',optimal_k)\n",
    "    t2 = datetime.now()  \n",
    "    print(t2-t1)\n",
    "    \n",
    "    t3 = datetime.now()  # start timer\n",
    "\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=rand_value)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    # Predict target variables for all possible triplets in each cluster\n",
    "    for j in range(optimal_k):\n",
    "        cluster_labels = kmeans.labels_ == j\n",
    "        cluster_y = y[cluster_labels]\n",
    "        triplet_list = list(itertools.product([0, 1], repeat=3))\n",
    "        print(len(triplet_list))\n",
    "        for triplet in triplet_list:\n",
    "            predicted_y = np.array([triplet for _ in range(sum(cluster_labels))])\n",
    "            for label in y.columns:\n",
    "                hamming_dist = hamming_loss(cluster_y[label].astype(str), predicted_y[:, y.columns.get_loc(label)].astype(str))\n",
    "                hamming_distances[label].append(hamming_dist)            \n",
    "#             hamming_distances[y.columns[0]].append(hamming_dist[0])\n",
    "#             hamming_distances[y.columns[1]].append(hamming_dist[1])\n",
    "#             hamming_distances[y.columns[2]].append(hamming_dist[2])\n",
    "    t4 = datetime.now()  # start timer\n",
    "    print(t4-t3)\n",
    "\n",
    "\n",
    "            # Calculate the mean and standard deviation of the Hamming distances for each column\n",
    "mean_dict = {}\n",
    "std_dict = {}\n",
    "for label in y.columns:\n",
    "    mean_dist = np.mean(hamming_distances[label])\n",
    "    std_dist = np.std(hamming_distances[label])\n",
    "    mean_dict[label] = mean_dist\n",
    "    std_dict[label] = std_dist\n",
    "    print(\"Label:\", label)\n",
    "    print(\"Mean Hamming distance:\", mean_dist)\n",
    "    print(\"Standard deviation of Hamming distance:\", std_dist)\n",
    "\n",
    "# Calculate overall mean and standard deviation of Hamming distances for each label\n",
    "mean_hamming_distances = sum(mean_dict.values())/len(mean_dict)\n",
    "sd_hamming_distances = sum(std_dict.values())/len(std_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Hamming distance: \", 1-mean_hamming_distances)\n",
    "print(\"Standard deviation of Hamming distance: \", sd_hamming_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The optimal K is: 4\n",
      "Iteration 1 - Hamming distance: 0.665, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 2 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 3 - Hamming distance: 0.736, Hamming loss: 0.245, Hamming score: 0.755\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 4 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 5 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 6 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 7 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 8 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 9 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 10 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 11 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 12 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 13 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 14 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 15 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 16 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 17 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 18 - Hamming distance: 0.701, Hamming loss: 0.234, Hamming score: 0.766\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 19 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 20 - Hamming distance: 0.665, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 21 - Hamming distance: 0.702, Hamming loss: 0.234, Hamming score: 0.766\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 22 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 23 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 24 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 25 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 26 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 27 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 28 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 29 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 30 - Hamming distance: 0.666, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 31 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 32 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 33 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 34 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 35 - Hamming distance: 0.558, Hamming loss: 0.186, Hamming score: 0.814\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 36 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 37 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 38 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 39 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 40 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 41 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 42 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 43 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 44 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 45 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 46 - Hamming distance: 0.665, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 47 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 48 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 49 - Hamming distance: 0.840, Hamming loss: 0.280, Hamming score: 0.720\n",
      "\n",
      "The optimal K is: 4\n",
      "Iteration 50 - Hamming distance: 0.667, Hamming loss: 0.222, Hamming score: 0.778\n",
      "Hamming score std for all possible triplets: 0.004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6653231410701876,\n",
       " 0.66726893676164,\n",
       " 0.7357887421820709,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.6674079221681724,\n",
       " 0.66726893676164,\n",
       " 0.6674079221681724,\n",
       " 0.66726893676164,\n",
       " 0.7011813759555247,\n",
       " 0.66726893676164,\n",
       " 0.6653231410701876,\n",
       " 0.7021542738012508,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.6674079221681724,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.6674079221681724,\n",
       " 0.66726893676164,\n",
       " 0.6664350243224462,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.5581653926337734,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.6653231410701876,\n",
       " 0.66726893676164,\n",
       " 0.66726893676164,\n",
       " 0.8401667824878388,\n",
       " 0.66726893676164]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def get_optimal_k(num_clusters, X, random_state):\n",
    "    optimalK, max_score = 2, 0\n",
    "    num_clusters_new = num_clusters + 1\n",
    "    for n in range(2, num_clusters_new):\n",
    "        clusterer = KMeans(n_clusters=n, random_state=random_state)\n",
    "        cluster_labels = clusterer.fit_predict(X)\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        if silhouette_avg > max_score:\n",
    "            optimalK = n\n",
    "            max_score = silhouette_avg\n",
    "    print(f\"\\nThe optimal K is: {optimalK}\")\n",
    "    return optimalK\n",
    "\n",
    "def MajorityLabels(optimalK, cluster_labels, Y):\n",
    "    cluster_major = pd.DataFrame(columns=Y.columns)\n",
    "    for c in range(optimalK):\n",
    "        cluster_idx, = np.where(cluster_labels == c)\n",
    "        cluster_samples = Y.iloc[cluster_idx, :]\n",
    "        row = []\n",
    "        for label in Y.columns:\n",
    "            cur_major = cluster_samples.loc[:, label].value_counts().index[0]\n",
    "            row.append(cur_major)\n",
    "        cluster_major.loc[c] = row\n",
    "    return cluster_major\n",
    "\n",
    "def calculation(cluster_major, cluster_labels, Y):\n",
    "    missclf_labels = 0\n",
    "    for c in range(len(cluster_major)):\n",
    "        cluster_idx, = np.where(cluster_labels == c)\n",
    "        for label in Y.loc[cluster_idx].values:\n",
    "            miss = (label != cluster_major.loc[c].values)\n",
    "            missclf_labels += np.sum(miss)\n",
    "    hamming_dist = missclf_labels / Y.shape[0]\n",
    "    hamming_loss = missclf_labels / (Y.shape[0] * Y.shape[1])\n",
    "    return hamming_dist, hamming_loss\n",
    "\n",
    "def run_monte_carlo(df, iterations):\n",
    "    hamming_distances = []\n",
    "    hamming_losses = []\n",
    "    hamming_scores = []\n",
    "    for i in range(iterations):\n",
    "        optimal_k = get_optimal_k(50, df.iloc[:, :-4], i)\n",
    "        clusterer = KMeans(n_clusters=optimal_k, random_state=i)\n",
    "        cluster_labels = clusterer.fit_predict(df.iloc[:, :-4])\n",
    "        cluster_major = MajorityLabels(optimal_k, cluster_labels, df.iloc[:, -4:-1])\n",
    "        hamming_distance, hamming_loss_val = calculation(cluster_major, cluster_labels, df.iloc[:, -4:-1])\n",
    "        hamming_score = 1 - hamming_loss_val\n",
    "        hamming_distances.append(hamming_distance)\n",
    "        hamming_losses.append(hamming_loss_val)\n",
    "        hamming_scores.append(hamming_score)\n",
    "        print(f\"Iteration {i+1} - Hamming distance: {hamming_distance:.3f}, Hamming loss: {hamming_loss_val:.3f}, Hamming score: {hamming_score:.3f}\")\n",
    "    \n",
    "    # calculate hamming score std for all possible triplets\n",
    "    hamming_score_std = []\n",
    "    for i in range(len(hamming_scores)-2):\n",
    "        for j in range(i+1, len(hamming_scores)-1):\n",
    "            for k in range(j+1, len(hamming_scores)):\n",
    "                triplet = [hamming_scores[i], hamming_scores[j], hamming_scores[k]]\n",
    "                hamming_score_std.append(np.std(triplet))\n",
    "    print(f\"Hamming score std for all possible triplets: {np.mean(hamming_score_std):.3f}\")\n",
    "    \n",
    "    return hamming_distances\n",
    "run_monte_carlo(frogs,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6711688672689368\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'majority_triplets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m mean_value \u001b[38;5;241m=\u001b[39m statistics\u001b[38;5;241m.\u001b[39mmean(lst)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(mean_value)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmajority_triplets\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'majority_triplets' is not defined"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "lst = [0.6653231410701876, 0.66726893676164, 0.7357887421820709, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.6674079221681724, 0.66726893676164, 0.6674079221681724, 0.66726893676164, 0.7011813759555247, 0.66726893676164, 0.6653231410701876, 0.7021542738012508, 0.66726893676164, 0.66726893676164, 0.6674079221681724, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.6674079221681724, 0.66726893676164, 0.6664350243224462, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.5581653926337734, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.66726893676164, 0.6653231410701876, 0.66726893676164, 0.66726893676164, 0.8401667824878388, 0.66726893676164]\n",
    "\n",
    "mean_value = statistics.mean(lst)\n",
    "print(mean_value)\n",
    "majority_triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Determine which family is the majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration :  0\n",
      "optimal_k is: 4\n",
      "4\n",
      "Iteration :  1\n",
      "optimal_k is: 4\n",
      "8\n",
      "Iteration :  2\n",
      "optimal_k is: 4\n",
      "12\n",
      "Iteration :  3\n",
      "optimal_k is: 4\n",
      "16\n",
      "Iteration :  4\n",
      "optimal_k is: 4\n",
      "20\n",
      "Iteration :  5\n",
      "optimal_k is: 4\n",
      "24\n",
      "Iteration :  6\n",
      "optimal_k is: 4\n",
      "28\n",
      "Iteration :  7\n",
      "optimal_k is: 4\n",
      "32\n",
      "Iteration :  8\n",
      "optimal_k is: 4\n",
      "36\n",
      "Iteration :  9\n",
      "optimal_k is: 4\n",
      "40\n",
      "Iteration :  10\n",
      "optimal_k is: 4\n",
      "44\n",
      "Iteration :  11\n",
      "optimal_k is: 4\n",
      "48\n",
      "Iteration :  12\n",
      "optimal_k is: 4\n",
      "52\n",
      "Iteration :  13\n",
      "optimal_k is: 4\n",
      "56\n",
      "Iteration :  14\n",
      "optimal_k is: 4\n",
      "60\n",
      "Iteration :  15\n",
      "optimal_k is: 4\n",
      "64\n",
      "Iteration :  16\n",
      "optimal_k is: 4\n",
      "68\n",
      "Iteration :  17\n",
      "optimal_k is: 4\n",
      "72\n",
      "Iteration :  18\n",
      "optimal_k is: 4\n",
      "76\n",
      "Iteration :  19\n",
      "optimal_k is: 4\n",
      "80\n",
      "Iteration :  20\n",
      "optimal_k is: 4\n",
      "84\n",
      "Iteration :  21\n",
      "optimal_k is: 4\n",
      "88\n",
      "Iteration :  22\n",
      "optimal_k is: 4\n",
      "92\n",
      "Iteration :  23\n",
      "optimal_k is: 4\n",
      "96\n",
      "Iteration :  24\n",
      "optimal_k is: 4\n",
      "100\n",
      "Iteration :  25\n",
      "optimal_k is: 4\n",
      "104\n",
      "Iteration :  26\n",
      "optimal_k is: 4\n",
      "108\n",
      "Iteration :  27\n",
      "optimal_k is: 4\n",
      "112\n",
      "Iteration :  28\n",
      "optimal_k is: 4\n",
      "116\n",
      "Iteration :  29\n",
      "optimal_k is: 4\n",
      "120\n",
      "Iteration :  30\n",
      "optimal_k is: 4\n",
      "124\n",
      "Iteration :  31\n",
      "optimal_k is: 4\n",
      "128\n",
      "Iteration :  32\n",
      "optimal_k is: 4\n",
      "132\n",
      "Iteration :  33\n",
      "optimal_k is: 4\n",
      "136\n",
      "Iteration :  34\n",
      "optimal_k is: 4\n",
      "140\n",
      "Iteration :  35\n",
      "optimal_k is: 4\n",
      "144\n",
      "Iteration :  36\n",
      "optimal_k is: 4\n",
      "148\n",
      "Iteration :  37\n",
      "optimal_k is: 4\n",
      "152\n",
      "Iteration :  38\n",
      "optimal_k is: 4\n",
      "156\n",
      "Iteration :  39\n",
      "optimal_k is: 4\n",
      "160\n",
      "Iteration :  40\n",
      "optimal_k is: 4\n",
      "164\n",
      "Iteration :  41\n",
      "optimal_k is: 4\n",
      "168\n",
      "Iteration :  42\n",
      "optimal_k is: 4\n",
      "172\n",
      "Iteration :  43\n",
      "optimal_k is: 4\n",
      "176\n",
      "Iteration :  44\n",
      "optimal_k is: 4\n",
      "180\n",
      "Iteration :  45\n",
      "optimal_k is: 4\n",
      "184\n",
      "Iteration :  46\n",
      "optimal_k is: 4\n",
      "188\n",
      "Iteration :  47\n",
      "optimal_k is: 4\n",
      "192\n",
      "Iteration :  48\n",
      "optimal_k is: 4\n",
      "196\n",
      "Iteration :  49\n",
      "optimal_k is: 4\n",
      "200\n",
      "Label: Family\n",
      "Mean Hamming distance: 0.6328283530229326\n",
      "Standard deviation of Hamming distance: 0.2558928845174819\n",
      "Label: Genus\n",
      "Mean Hamming distance: 0.6775927727588602\n",
      "Standard deviation of Hamming distance: 0.26257840289996764\n",
      "Label: Species\n",
      "Mean Hamming distance: 0.7582070882557332\n",
      "Standard deviation of Hamming distance: 0.26673968312660973\n"
     ]
    }
   ],
   "source": [
    "# Perform the procedure 50 times\n",
    "n_iter = 50\n",
    "sil_scores = []\n",
    "majority_triplets = []\n",
    "hamming_distances = {label: [] for label in y.columns}\n",
    "for i in range(n_iter):\n",
    "    # Find optimal number of clusters using Silhouette score\n",
    "    \n",
    "\n",
    "    for k in range(2,21):\n",
    "        rand_value=random.randint(0, 100)\n",
    "        k_means = KMeans(n_clusters=k,init='k-means++',random_state=rand_value).fit(X)\n",
    "        labels = k_means.labels_\n",
    "        silh_avg.update({k:(metrics.silhouette_score(X, labels))})\n",
    "   \n",
    "    print(\"Iteration : \",i)\n",
    "    #print(\"Average Silhoutte score values : \",silh_avg)\n",
    "\n",
    "    # Get optimal k and build K-means clustering with it\n",
    "\n",
    "\n",
    "    optimal_k = max(silh_avg,key=silh_avg.get)\n",
    "    \n",
    "    rand_value=random.randint(0, 900)\n",
    "    print('optimal_k is:',optimal_k)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=rand_value)\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "\n",
    "    # Determine majority triplet for each cluster\n",
    "    for j in range(optimal_k):\n",
    "        cluster_labels = kmeans.labels_ == j\n",
    "        cluster_y = y[cluster_labels]\n",
    "        majority_triplet = cluster_y.mode().iloc[0].values\n",
    "        majority_triplets.append(majority_triplet)\n",
    "    print(len(majority_triplets))\n",
    "\n",
    "    # Predict target variables using majority triplet\n",
    "    predicted_y = np.array([majority_triplets[label] for label in kmeans.labels_])\n",
    "    # Calculate Hamming distance for each column separately\n",
    "    for label in y.columns:\n",
    "        hamming_dist = hamming_loss(y[label], predicted_y[:, y.columns.get_loc(label)])\n",
    "        hamming_distances[label].append(hamming_dist)\n",
    "# Calculate the mean and standard deviation of the Hamming distances for each column\n",
    "mean_dict = {}\n",
    "std_dict = {}\n",
    "for label in y.columns:\n",
    "    mean_dist = np.mean(hamming_distances[label])\n",
    "    std_dist = np.std(hamming_distances[label])\n",
    "    mean_dict[label] = mean_dist\n",
    "    std_dict[label] = std_dist\n",
    "    print(\"Label:\", label)\n",
    "    print(\"Mean Hamming distance:\", mean_dist)\n",
    "    print(\"Standard deviation of Hamming distance:\", std_dist)\n",
    "    \n",
    "# Calculate overall mean and standard deviation of Hamming distances for each label\n",
    "mean_hamming_distances = sum(mean_dict.values())/len(mean_dict)\n",
    "sd_hamming_distances = sum(std_dict.values())/len(std_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Calculate the average Hamming distance, Hamming score, and Hamming loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the answer is above the triplets, doing another computation is too time consuming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) draw a complete dandrogram based on the dissimilarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haoran Zhang\\AppData\\Local\\Temp\\ipykernel_25940\\3178099325.py:12: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  Z = linkage(dis_matrix, method='complete')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the distance matrix\n",
    "dis_matrix = np.array([[0, 0.3, 0.4, 0.7],\n",
    "                  [0.3, 0, 0.5, 0.8],\n",
    "                  [0.4, 0.5, 0.0, 0.45],\n",
    "                  [0.7, 0.8, 0.45, 0.0]])\n",
    "                  \n",
    "# Compute hierarchical clustering and plot the dendrogram\n",
    "Z = linkage(dis_matrix, method='complete')\n",
    "fig = plt.figure(figsize=(5,3))\n",
    "dn = dendrogram(Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Repeat (a), this time using simple linkage clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haoran Zhang\\AppData\\Local\\Temp\\ipykernel_25940\\1062976236.py:2: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  linkage_matrix = linkage(dis_matrix, method='single')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'icoord': [[25.0, 25.0, 35.0, 35.0],\n",
       "  [15.0, 15.0, 30.0, 30.0],\n",
       "  [5.0, 5.0, 22.5, 22.5]],\n",
       " 'dcoord': [[0.0, 0.4472135954999579, 0.4472135954999579, 0.0],\n",
       "  [0.0, 0.65, 0.65, 0.4472135954999579],\n",
       "  [0.0, 0.7648529270389177, 0.7648529270389177, 0.65]],\n",
       " 'ivl': ['4', '3', '1', '2'],\n",
       " 'leaves': [3, 2, 0, 1],\n",
       " 'color_list': ['C1', 'C0', 'C0'],\n",
       " 'leaves_color_list': ['C0', 'C0', 'C1', 'C1']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute hierarchical clustering and plot the dendrogram\n",
    "linkage_matrix = linkage(dis_matrix, method='single')\n",
    "dendrogram(linkage_matrix, color_threshold=0.5, labels=['1', '2', '3', '4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Suppose that we cut the dendrogram obtained in (a) such that two clusters result. Which observations are in each cluster ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue '>  In this case, we have clusters (1,2) and (3,4).<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Suppose that we cut the dendrogram obtained in (b) such that two clusters result. Which observations are in each cluster ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue '>  In this case, we have clusters ((1,2),3) and (4).<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Draw a dendrogram that is equivalent to the dendrogram in (a), for which two or more of the leaves are repositioned, but for which the meaning of the dendrogram is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Haoran Zhang\\AppData\\Local\\Temp\\ipykernel_25940\\4063453816.py:1: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  Z = linkage(dis_matrix, method='complete')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Z = linkage(dis_matrix, method='complete')\n",
    "plt.figure(figsize=(8, 6))\n",
    "dendrogram(Z, labels=[1,0,3,2])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c20c2d94d2527936fe0f3a300eb11db30fed84423423838e2f93b74eb7aaebc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
