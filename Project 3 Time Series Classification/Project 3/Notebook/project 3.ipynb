{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Time Series Classification Part 1: Feature Creation/Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the AReM Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative path to the AReM folder\n",
    "base_path = \"../Data/AReM\"\n",
    "\n",
    "# List of activities (folders)\n",
    "activities = [\"bending1\", \"bending2\", \"cycling\", \"lying\", \"sitting\", \"standing\", \"walking\"]\n",
    "\n",
    "for activity in activities:\n",
    "    activity_path = os.path.join(base_path, activity)\n",
    "    \n",
    "    # Get a list of all dataset files in the current activity folder\n",
    "    datasets = [f for f in os.listdir(activity_path) if f.startswith(\"dataset\")]\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        file_path = os.path.join(activity_path, dataset)\n",
    "        \n",
    "        # Check if the current file is bending2/dataset4.csv because dataset4 for bending2 is space separated\n",
    "        is_special_file = (activity == \"bending2\" and dataset == \"dataset4.csv\")\n",
    "        \n",
    "        try:\n",
    "            if is_special_file:\n",
    "                # Read the file as space-separated\n",
    "                df = pd.read_csv(file_path, skiprows=4, sep=' ', header=None)\n",
    "                # Rename columns\n",
    "                df.columns = ['time', 'avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
    "            else:\n",
    "                # Load the dataset into a pandas DataFrame, assuming the first 4 rows are metadata\n",
    "                df = pd.read_csv(file_path, skiprows=4)\n",
    "            \n",
    "            # Display the first few rows of the DataFrame\n",
    "            print(f\"First few rows for {activity} - {dataset}:\")\n",
    "            print(df.head())\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "        except pd.errors.ParserError:\n",
    "            print(f\"Error reading {file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Test and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected datasets for bending1: ['dataset1', 'dataset2', 'dataset3', 'dataset4', 'dataset5', 'dataset6', 'dataset7']\n",
      "Test datasets for bending1: ['dataset1', 'dataset2']\n",
      "Train datasets for bending1: ['dataset3', 'dataset4', 'dataset5', 'dataset6', 'dataset7']\n",
      "Detected datasets for bending2: ['dataset1', 'dataset2', 'dataset3', 'dataset4', 'dataset5', 'dataset6']\n",
      "Test datasets for bending2: ['dataset1', 'dataset2']\n",
      "Train datasets for bending2: ['dataset3', 'dataset4', 'dataset5', 'dataset6']\n",
      "Detected datasets for cycling: ['dataset1', 'dataset10', 'dataset11', 'dataset12', 'dataset13', 'dataset14', 'dataset15', 'dataset2', 'dataset3', 'dataset4', 'dataset5', 'dataset6', 'dataset7', 'dataset8', 'dataset9']\n",
      "Test datasets for cycling: ['dataset1', 'dataset2', 'dataset3']\n",
      "Train datasets for cycling: ['dataset10', 'dataset11', 'dataset12', 'dataset13', 'dataset14', 'dataset15', 'dataset4', 'dataset5', 'dataset6', 'dataset7', 'dataset8', 'dataset9']\n",
      "Detected datasets for lying: ['dataset1', 'dataset10', 'dataset11', 'dataset12', 'dataset13', 'dataset14', 'dataset15', 'dataset2', 'dataset3', 'dataset4', 'dataset5', 'dataset6', 'dataset7', 'dataset8', 'dataset9']\n",
      "Test datasets for lying: ['dataset1', 'dataset2', 'dataset3']\n",
      "Train datasets for lying: ['dataset10', 'dataset11', 'dataset12', 'dataset13', 'dataset14', 'dataset15', 'dataset4', 'dataset5', 'dataset6', 'dataset7', 'dataset8', 'dataset9']\n",
      "Detected datasets for sitting: ['dataset1', 'dataset10', 'dataset11', 'dataset12', 'dataset13', 'dataset14', 'dataset15', 'dataset2', 'dataset3', 'dataset4', 'dataset5', 'dataset6', 'dataset7', 'dataset8', 'dataset9']\n",
      "Test datasets for sitting: ['dataset1', 'dataset2', 'dataset3']\n",
      "Train datasets for sitting: ['dataset10', 'dataset11', 'dataset12', 'dataset13', 'dataset14', 'dataset15', 'dataset4', 'dataset5', 'dataset6', 'dataset7', 'dataset8', 'dataset9']\n",
      "Detected datasets for standing: ['dataset1', 'dataset10', 'dataset11', 'dataset12', 'dataset13', 'dataset14', 'dataset15', 'dataset2', 'dataset3', 'dataset4', 'dataset5', 'dataset6', 'dataset7', 'dataset8', 'dataset9']\n",
      "Test datasets for standing: ['dataset1', 'dataset2', 'dataset3']\n",
      "Train datasets for standing: ['dataset10', 'dataset11', 'dataset12', 'dataset13', 'dataset14', 'dataset15', 'dataset4', 'dataset5', 'dataset6', 'dataset7', 'dataset8', 'dataset9']\n",
      "Detected datasets for walking: ['dataset1', 'dataset10', 'dataset11', 'dataset12', 'dataset13', 'dataset14', 'dataset15', 'dataset2', 'dataset3', 'dataset4', 'dataset5', 'dataset6', 'dataset7', 'dataset8', 'dataset9']\n",
      "Test datasets for walking: ['dataset1', 'dataset2', 'dataset3']\n",
      "Train datasets for walking: ['dataset10', 'dataset11', 'dataset12', 'dataset13', 'dataset14', 'dataset15', 'dataset4', 'dataset5', 'dataset6', 'dataset7', 'dataset8', 'dataset9']\n",
      "\n",
      "Test Data Paths:\n",
      "../Data/AReM\\bending1\\dataset1.csv\n",
      "../Data/AReM\\bending1\\dataset2.csv\n",
      "../Data/AReM\\bending2\\dataset1.csv\n",
      "../Data/AReM\\bending2\\dataset2.csv\n",
      "../Data/AReM\\cycling\\dataset1.csv\n",
      "../Data/AReM\\cycling\\dataset2.csv\n",
      "../Data/AReM\\cycling\\dataset3.csv\n",
      "../Data/AReM\\lying\\dataset1.csv\n",
      "../Data/AReM\\lying\\dataset2.csv\n",
      "../Data/AReM\\lying\\dataset3.csv\n",
      "../Data/AReM\\sitting\\dataset1.csv\n",
      "../Data/AReM\\sitting\\dataset2.csv\n",
      "../Data/AReM\\sitting\\dataset3.csv\n",
      "../Data/AReM\\standing\\dataset1.csv\n",
      "../Data/AReM\\standing\\dataset2.csv\n",
      "../Data/AReM\\standing\\dataset3.csv\n",
      "../Data/AReM\\walking\\dataset1.csv\n",
      "../Data/AReM\\walking\\dataset2.csv\n",
      "../Data/AReM\\walking\\dataset3.csv\n",
      "\n",
      "Train Data Paths:\n",
      "../Data/AReM\\bending1\\dataset3.csv\n",
      "../Data/AReM\\bending1\\dataset4.csv\n",
      "../Data/AReM\\bending1\\dataset5.csv\n",
      "../Data/AReM\\bending1\\dataset6.csv\n",
      "../Data/AReM\\bending1\\dataset7.csv\n",
      "../Data/AReM\\bending2\\dataset3.csv\n",
      "../Data/AReM\\bending2\\dataset4.csv\n",
      "../Data/AReM\\bending2\\dataset5.csv\n",
      "../Data/AReM\\bending2\\dataset6.csv\n",
      "../Data/AReM\\cycling\\dataset10.csv\n",
      "../Data/AReM\\cycling\\dataset11.csv\n",
      "../Data/AReM\\cycling\\dataset12.csv\n",
      "../Data/AReM\\cycling\\dataset13.csv\n",
      "../Data/AReM\\cycling\\dataset14.csv\n",
      "../Data/AReM\\cycling\\dataset15.csv\n",
      "../Data/AReM\\cycling\\dataset4.csv\n",
      "../Data/AReM\\cycling\\dataset5.csv\n",
      "../Data/AReM\\cycling\\dataset6.csv\n",
      "../Data/AReM\\cycling\\dataset7.csv\n",
      "../Data/AReM\\cycling\\dataset8.csv\n",
      "../Data/AReM\\cycling\\dataset9.csv\n",
      "../Data/AReM\\lying\\dataset10.csv\n",
      "../Data/AReM\\lying\\dataset11.csv\n",
      "../Data/AReM\\lying\\dataset12.csv\n",
      "../Data/AReM\\lying\\dataset13.csv\n",
      "../Data/AReM\\lying\\dataset14.csv\n",
      "../Data/AReM\\lying\\dataset15.csv\n",
      "../Data/AReM\\lying\\dataset4.csv\n",
      "../Data/AReM\\lying\\dataset5.csv\n",
      "../Data/AReM\\lying\\dataset6.csv\n",
      "../Data/AReM\\lying\\dataset7.csv\n",
      "../Data/AReM\\lying\\dataset8.csv\n",
      "../Data/AReM\\lying\\dataset9.csv\n",
      "../Data/AReM\\sitting\\dataset10.csv\n",
      "../Data/AReM\\sitting\\dataset11.csv\n",
      "../Data/AReM\\sitting\\dataset12.csv\n",
      "../Data/AReM\\sitting\\dataset13.csv\n",
      "../Data/AReM\\sitting\\dataset14.csv\n",
      "../Data/AReM\\sitting\\dataset15.csv\n",
      "../Data/AReM\\sitting\\dataset4.csv\n",
      "../Data/AReM\\sitting\\dataset5.csv\n",
      "../Data/AReM\\sitting\\dataset6.csv\n",
      "../Data/AReM\\sitting\\dataset7.csv\n",
      "../Data/AReM\\sitting\\dataset8.csv\n",
      "../Data/AReM\\sitting\\dataset9.csv\n",
      "../Data/AReM\\standing\\dataset10.csv\n",
      "../Data/AReM\\standing\\dataset11.csv\n",
      "../Data/AReM\\standing\\dataset12.csv\n",
      "../Data/AReM\\standing\\dataset13.csv\n",
      "../Data/AReM\\standing\\dataset14.csv\n",
      "../Data/AReM\\standing\\dataset15.csv\n",
      "../Data/AReM\\standing\\dataset4.csv\n",
      "../Data/AReM\\standing\\dataset5.csv\n",
      "../Data/AReM\\standing\\dataset6.csv\n",
      "../Data/AReM\\standing\\dataset7.csv\n",
      "../Data/AReM\\standing\\dataset8.csv\n",
      "../Data/AReM\\standing\\dataset9.csv\n",
      "../Data/AReM\\walking\\dataset10.csv\n",
      "../Data/AReM\\walking\\dataset11.csv\n",
      "../Data/AReM\\walking\\dataset12.csv\n",
      "../Data/AReM\\walking\\dataset13.csv\n",
      "../Data/AReM\\walking\\dataset14.csv\n",
      "../Data/AReM\\walking\\dataset15.csv\n",
      "../Data/AReM\\walking\\dataset4.csv\n",
      "../Data/AReM\\walking\\dataset5.csv\n",
      "../Data/AReM\\walking\\dataset6.csv\n",
      "../Data/AReM\\walking\\dataset7.csv\n",
      "../Data/AReM\\walking\\dataset8.csv\n",
      "../Data/AReM\\walking\\dataset9.csv\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "base_path = \"../Data/AReM\"\n",
    "\n",
    "activities = [\"bending1\", \"bending2\", \"cycling\", \"lying\", \"sitting\", \"standing\", \"walking\"]\n",
    "\n",
    "test_data_paths = []\n",
    "train_data_paths = []\n",
    "\n",
    "for activity in activities:\n",
    "    activity_path = os.path.join(base_path, activity)\n",
    "    \n",
    "    # Check for the existence of the activity_path\n",
    "    if not os.path.exists(activity_path):\n",
    "        print(f\"Directory not found: {activity_path}\")\n",
    "        continue\n",
    "    \n",
    "    # List all datasets for the current activity and remove .csv extension\n",
    "    datasets = [f.replace('.csv', '') for f in os.listdir(activity_path) if f.startswith(\"dataset\")]\n",
    "    \n",
    "    # Debugging print to show detected datasets for each activity\n",
    "    print(f\"Detected datasets for {activity}: {datasets}\")\n",
    "\n",
    "    if activity in [\"bending1\", \"bending2\"]:\n",
    "        test_sets = [ds for ds in datasets if ds in [\"dataset1\", \"dataset2\"]]\n",
    "        train_sets = [ds for ds in datasets if ds not in [\"dataset1\", \"dataset2\"]]\n",
    "    else:\n",
    "        test_sets = [ds for ds in datasets if ds in [\"dataset1\", \"dataset2\", \"dataset3\"]]\n",
    "        train_sets = [ds for ds in datasets if ds not in [\"dataset1\", \"dataset2\", \"dataset3\"]]\n",
    "    \n",
    "    # Debugging prints to show which datasets are assigned to test and train for each activity\n",
    "    print(f\"Test datasets for {activity}: {test_sets}\")\n",
    "    print(f\"Train datasets for {activity}: {train_sets}\")\n",
    "\n",
    "    test_data_paths.extend([os.path.join(activity_path, ds + '.csv') for ds in test_sets])  # Added .csv back for the path\n",
    "    train_data_paths.extend([os.path.join(activity_path, ds + '.csv') for ds in train_sets])  # Added .csv back for the path\n",
    "\n",
    "print(\"\\nTest Data Paths:\")\n",
    "for path in test_data_paths:\n",
    "    print(path)\n",
    "\n",
    "print(\"\\nTrain Data Paths:\")\n",
    "for path in train_data_paths:\n",
    "    print(path)\n",
    "    \n",
    "print(len(train_data_paths)+len(test_data_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### i. Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Statistical Features:\n",
    "\n",
    "Mean: Average value of the time series.\n",
    "Median: Middle value of the time series.\n",
    "Mode: The value that appears most frequently in a data set.\n",
    "Standard Deviation: Measure of the amount of variation or dispersion in the values.\n",
    "Variance: Square of the standard deviation. It represents the variability from the average.\n",
    "Skewness: Measure of the asymmetry of the distribution of the time series.\n",
    "Kurtosis: Measure of the \"tailedness\" of the distribution in the time series.\n",
    "Range-Based Features:\n",
    "\n",
    "Minimum: The smallest value in the time series.\n",
    "Maximum: The largest value in the time series.\n",
    "Range: Difference between the maximum and the minimum.\n",
    "Quantiles: Values taken at regular intervals, e.g., 25th, 50th (median), and 75th percentiles.\n",
    "Frequency:\n",
    "\n",
    "Zero Crossing Rate: The rate at which the signal changes from positive to negative or vice versa.\n",
    "Mean Crossings: The rate at which the signal crosses its mean value.\n",
    "Signal Magnitude Area (SMA): Represents the cumulative magnitude of a time series.\n",
    "\n",
    "Energy Measures:\n",
    "\n",
    "Total Energy: Sum of the squared values.\n",
    "Signal Power: Average power of the signal.\n",
    "Shape-Based Features:\n",
    "\n",
    "Waveform Length: Sum of the absolute differences between adjacent data points.\n",
    "Slope: Directional information about the signal.\n",
    "Autocorrelation: Measures the similarity between observations as a function of the time lag between them.\n",
    "\n",
    "Entropy: Measure of unpredictability or randomness in the time series.\n",
    "\n",
    "Time Series Decomposition Components: Such as trends, seasonalities, and residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ii. Extraction\n",
    "1 -> avg_rss12\n",
    "2 -> var_rss12\n",
    "3 -> avg_rss13\n",
    "4 -> var_rss13\n",
    "5 -> avg_rss23\n",
    "6 -> var_rss23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     min1   max1      mean1  median1      std1  1st_quart1  3rd_quart1  min2  \\\n",
      "0   37.25  45.00  40.624792   40.500  1.475428       39.25     42.0000   0.0   \n",
      "1   38.00  45.67  42.812812   42.500  1.434054       42.00     43.6700   0.0   \n",
      "2   35.00  47.40  43.954500   44.330  1.557210       43.00     45.0000   0.0   \n",
      "3   33.00  47.75  42.179812   43.500  3.666840       39.15     45.0000   0.0   \n",
      "4   33.00  45.75  41.678063   41.750  2.241152       41.33     42.7500   0.0   \n",
      "..    ...    ...        ...      ...       ...         ...         ...   ...   \n",
      "83  20.75  46.25  34.763333   35.290  4.737266       31.67     38.2500   0.0   \n",
      "84  21.50  51.00  34.935812   35.500  4.641102       32.00     38.0625   0.0   \n",
      "85  18.33  47.67  34.333042   34.750  4.943612       31.25     38.0000   0.0   \n",
      "86  18.33  45.75  34.599875   35.125  4.726858       31.50     38.0000   0.0   \n",
      "87  15.50  43.67  34.225875   34.750  4.437168       31.25     37.2500   0.0   \n",
      "\n",
      "     max2     mean2  ...      std5  1st_quart5  3rd_quart5  min6   max6  \\\n",
      "0    1.30  0.358604  ...  2.186168     33.0000       36.00   0.0   1.92   \n",
      "1    1.22  0.372437  ...  1.993175     32.0000       34.50   0.0   3.11   \n",
      "2    1.70  0.426250  ...  1.997520     35.3625       36.50   0.0   1.79   \n",
      "3    3.00  0.696042  ...  3.845436     30.4575       36.33   0.0   2.18   \n",
      "4    2.83  0.535979  ...  2.408514     28.4575       31.25   0.0   1.79   \n",
      "..    ...       ...  ...       ...         ...         ...   ...    ...   \n",
      "83  12.68  4.223792  ...  3.171372     14.2500       18.33   0.0   9.39   \n",
      "84  12.21  4.115750  ...  3.188731     14.2375       18.25   0.0  10.21   \n",
      "85  12.48  4.396958  ...  2.997366     13.7500       18.00   0.0   8.01   \n",
      "86  15.37  4.398833  ...  2.902659     14.0000       18.25   0.0   8.86   \n",
      "87  17.24  4.354500  ...  2.989801     14.3300       18.25   0.0   9.42   \n",
      "\n",
      "       mean6  median6      std6  1st_quart6  3rd_quart6  \n",
      "0   0.570583    0.430  0.582308        0.00      1.3000  \n",
      "1   0.571083    0.430  0.600383        0.00      1.3000  \n",
      "2   0.493292    0.430  0.512971        0.00      0.9400  \n",
      "3   0.613521    0.500  0.523771        0.00      1.0000  \n",
      "4   0.383292    0.430  0.388759        0.00      0.5000  \n",
      "..       ...      ...       ...         ...         ...  \n",
      "83  3.288271    3.270  1.645811        2.05      4.3050  \n",
      "84  3.280021    3.015  1.699145        2.12      4.5000  \n",
      "85  3.261583    2.980  1.615604        2.05      4.3200  \n",
      "86  3.289542    3.015  1.678418        2.12      4.2600  \n",
      "87  3.479542    3.270  1.759311        2.24      4.5375  \n",
      "\n",
      "[88 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# Setting the path to the dataset\n",
    "base_path = \"../Data/AReM/\"\n",
    "\n",
    "# Assuming you have a list of the activities you're interested in\n",
    "activities = [\"bending1\", \"bending2\", \"cycling\", \"lying\", \"sitting\", \"standing\", \"walking\"]\n",
    "\n",
    "# Collect all the paths of the training data\n",
    "train_data_paths = []\n",
    "\n",
    "for activity in activities:\n",
    "    activity_path = os.path.join(base_path, activity)\n",
    "    for filename in os.listdir(activity_path):\n",
    "        # Assuming 'dataset' in filename indicates it's a data file\n",
    "        if \"dataset\" in filename:\n",
    "            train_data_paths.append(os.path.join(activity_path, filename))\n",
    "\n",
    "all_features = []\n",
    "\n",
    "for data_path in train_data_paths:\n",
    "    data = pd.read_csv(data_path, skiprows=4)\n",
    "    instance_features = []\n",
    "    \n",
    "    for idx, column in enumerate(['avg_rss12','var_rss12','avg_rss13','var_rss13','avg_rss23','var_rss23'], start=1):\n",
    "        values = data[column].values\n",
    "        features = [\n",
    "            np.min(values),\n",
    "            np.max(values),\n",
    "            np.mean(values),\n",
    "            np.median(values),\n",
    "            np.std(values),  # This is the standard deviation\n",
    "            np.percentile(values, 25),\n",
    "            np.percentile(values, 75),\n",
    "        ]\n",
    "        instance_features.extend(features)\n",
    "        \n",
    "    all_features.append(instance_features)\n",
    "\n",
    "# Create DataFrame columns dynamically based on the features and time series\n",
    "columns = []\n",
    "for i in range(1, 7):  # For each of the six time series\n",
    "    columns.extend([f'min{i}', f'max{i}', f'mean{i}', f'median{i}', f'std{i}', f'1st_quart{i}', f'3rd_quart{i}'])\n",
    "\n",
    "df_features = pd.DataFrame(all_features, columns=columns)\n",
    "print(df_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iii. Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Estimated Std Dev  5th Percentile  95th Percentile\n",
      "avg_rss12           4.441798        4.203300         4.671170\n",
      "var_rss12           2.518991        2.335537         2.697440\n",
      "avg_rss13           2.812274        2.659772         2.951090\n",
      "var_rss13           1.730792        1.635415         1.819587\n",
      "avg_rss23           2.992920        2.821113         3.156677\n",
      "var_rss23           1.761146        1.659445         1.858199\n"
     ]
    }
   ],
   "source": [
    "# Define function to compute bootstrap confidence intervals for the standard deviation\n",
    "def bootstrap_std_conf_interval(data, num_samples=10000, alpha=0.1):\n",
    "    np.random.seed(42)  # for reproducibility\n",
    "    bootstrap_stds = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Draw a random sample of data with replacement\n",
    "        sample = data.sample(len(data), replace=True)\n",
    "        \n",
    "        # Store the standard deviation of this sample\n",
    "        bootstrap_stds.append(sample.std())\n",
    "\n",
    "    # Determine percentiles for confidence interval\n",
    "    lower = (alpha/2) * 100\n",
    "    upper = (1 - (alpha/2)) * 100\n",
    "    conf_intervals = np.percentile(bootstrap_stds, [lower, upper])\n",
    "\n",
    "    return conf_intervals\n",
    "\n",
    "features = ['avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
    "results = {}\n",
    "\n",
    "# Compute bootstrap confidence intervals for the standard deviation of each feature\n",
    "for feature in features:\n",
    "    conf_intervals = bootstrap_std_conf_interval(df[feature])\n",
    "    results[feature] = {\n",
    "        'Estimated Std Dev': df[feature].std(),\n",
    "        '5th Percentile': conf_intervals[0],\n",
    "        '95th Percentile': conf_intervals[1]\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).transpose()\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### iv. Select Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean: The mean provides the central tendency of the data. It gives you an idea of the average value of your time series over a period. This could be particularly useful to understand the baseline or the average level of the activity. If there's significant variation between activities in their average levels, this feature would be valuable.\n",
    "\n",
    "Min/Max: The minimum and maximum values give the range of the data. They can capture extreme values or outliers and are especially relevant if sudden spikes or drops in the time series are important for differentiation. For example, the maximum value might help distinguish between walking and running if running produces higher peak values. Similarly, the minimum value might be crucial if you're interested in the lowest point of the signal.\n",
    "\n",
    "Standard Deviation: This feature is valuable for understanding the variability or dispersion of the data around the mean. If the activities you're trying to classify or differentiate have different levels of variability or consistency, then standard deviation would be an important feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Linear Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training data, the cubic regression model will almost always have a lower (or at worst, the same) RSS compared to the linear regression model. This is because the cubic model includes the linear term and has the added flexibility of the quadratic and cubic terms, allowing it to fit the data more closely, even if the true relationship is purely linear. In essence, the cubic model can reduce the residuals by adapting its shape more closely to the data points, even if some of this \"fit\" is just capturing noise.\n",
    "\n",
    "So, we would expect the RSS for the cubic regression to be lower than (or at worst, equal to) the RSS for the linear regression when trained on the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Linear Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the true relationship between X and Y is linear, a cubic regression model, even though it might fit the training data better (as explained in part (a)), may not generalize well to new, unseen data. This phenomenon is commonly referred to as overfitting.\n",
    "\n",
    "Overfitting happens when a model is too flexible and captures not just the underlying relationship but also the random noise in the training data. When the model is then applied to test data (or any new data outside the training set), it might produce predictions that are off because it's reacting to patterns (noise) that don't exist in the new dataset.\n",
    "\n",
    "For the test RSS:\n",
    "\n",
    "The linear regression model is likely to generalize better to new data if the true relationship is indeed linear because it captures just the linear relationship without being influenced by the noise in the training data.\n",
    "\n",
    "The cubic regression model, on the other hand, might perform worse on the test data because it may have adapted too closely to the idiosyncrasies (including noise) of the training data, leading to poorer generalization.\n",
    "\n",
    "Therefore, we would expect the test RSS for the cubic regression to be higher (indicating worse performance) than the test RSS for the linear regression if the true relationship is linear. However, it's worth noting that this isn't guaranteed and depends on the nature of the noise and the specific data at hand.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Not Linear Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model is constrained to model only a linear relationship between X and Y. If the true relationship is not linear, this model may not fit the data very well.\n",
    "\n",
    "The cubic regression model, on the other hand, has the potential to fit both linear and certain nonlinear relationships (up to a cubic polynomial). This added flexibility means it can adapt to a wider variety of relationships between X and Y compared to the linear model.\n",
    "\n",
    "Considering the training RSS:\n",
    "\n",
    "If the relationship is indeed not linear, the cubic regression will likely be able to capture more of this nonlinear pattern, leading to a lower training RSS compared to the linear regression.\n",
    "Therefore, we would expect the training RSS for the cubic regression to be lower than the training RSS for the linear regression when the true relationship is not linear. However, the extent to which the cubic model outperforms the linear model depends on the nature and degree of the nonlinearity in the true relationship. If the true relationship is very close to linear, the difference in RSS might be small; if it's strongly nonlinear (and particularly if it aligns with the kinds of nonlinearities a cubic polynomial can capture), the difference might be substantial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Not Linear Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the nonlinearity in the true relationship between X and Y can be well-approximated by a cubic polynomial, then the cubic regression is likely to outperform the linear regression on both the training and test datasets. This means that the test RSS for the cubic regression would be expected to be lower than that of the linear regression because the cubic regression would generalize better to new data by capturing the underlying nonlinear relationship.\n",
    "\n",
    "However, if the true nonlinear relationship is more complex or is of a different form than what can be captured by a cubic polynomial, the cubic regression model might still overfit the training data. This overfitting could result in a higher test RSS for the cubic regression compared to the linear regression, even if the latter is underfitting. This is because the cubic model could be capturing noise or idiosyncratic patterns in the training data that don't generalize well to the test data.\n",
    "\n",
    "In conclusion, with only the information that the relationship is not linear (but no clarity on the nature of the nonlinearity), it's difficult to definitively state which model will have a lower test RSS. While the cubic model has the potential to better capture certain nonlinearities, its increased flexibility also means it has a higher risk of overfitting, which can adversely affect test RSS. Thus, there's not enough information to definitively tell which model will have a lower test RSS without knowing more about the nature of the true relationship or without actually evaluating both models on test data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.435px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "3c20c2d94d2527936fe0f3a300eb11db30fed84423423838e2f93b74eb7aaebc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
